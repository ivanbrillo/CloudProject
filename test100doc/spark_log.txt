2025-05-31 05:59:40,115 INFO spark.SparkContext: Running Spark version 3.4.4
2025-05-31 05:59:40,502 INFO resource.ResourceUtils: ==============================================================
2025-05-31 05:59:40,502 INFO resource.ResourceUtils: No custom resources configured for spark.driver.
2025-05-31 05:59:40,503 INFO resource.ResourceUtils: ==============================================================
2025-05-31 05:59:40,503 INFO spark.SparkContext: Submitted application: invertedIndex
2025-05-31 05:59:40,545 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-31 05:59:40,570 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor
2025-05-31 05:59:40,574 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0
2025-05-31 05:59:40,663 INFO spark.SecurityManager: Changing view acls to: hadoop
2025-05-31 05:59:40,664 INFO spark.SecurityManager: Changing modify acls to: hadoop
2025-05-31 05:59:40,664 INFO spark.SecurityManager: Changing view acls groups to: 
2025-05-31 05:59:40,665 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-05-31 05:59:40,665 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
2025-05-31 05:59:41,027 INFO util.Utils: Successfully started service 'sparkDriver' on port 46551.
2025-05-31 05:59:41,105 INFO spark.SparkEnv: Registering MapOutputTracker
2025-05-31 05:59:41,167 INFO spark.SparkEnv: Registering BlockManagerMaster
2025-05-31 05:59:41,212 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-31 05:59:41,213 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-31 05:59:41,309 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-31 05:59:41,359 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-88020a6d-e4aa-4505-8562-535eafeb523b
2025-05-31 05:59:41,397 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MiB
2025-05-31 05:59:41,530 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2025-05-31 05:59:41,602 INFO util.log: Logging initialized @4435ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-05-31 05:59:41,776 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2025-05-31 05:59:41,805 INFO server.Server: jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 1.8.0_452-8u452-ga~us1-0ubuntu1~22.04-b09
2025-05-31 05:59:41,839 INFO server.Server: Started @4675ms
2025-05-31 05:59:41,896 INFO server.AbstractConnector: Started ServerConnector@690170d4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-05-31 05:59:41,896 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-31 05:59:42,045 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@272805eb{/,null,AVAILABLE,@Spark}
2025-05-31 05:59:43,042 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.183:8032
2025-05-31 05:59:44,630 INFO conf.Configuration: resource-types.xml not found
2025-05-31 05:59:44,632 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-05-31 05:59:44,660 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container)
2025-05-31 05:59:44,661 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
2025-05-31 05:59:44,661 INFO yarn.Client: Setting up container launch context for our AM
2025-05-31 05:59:44,665 INFO yarn.Client: Setting up the launch environment for our AM container
2025-05-31 05:59:44,683 INFO yarn.Client: Preparing resources for our AM container
2025-05-31 05:59:44,745 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2025-05-31 05:59:47,063 INFO yarn.Client: Uploading resource file:/tmp/spark-5b331578-8b57-4254-9e1b-09bfccbcb821/__spark_libs__1861179588220582646.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2003/__spark_libs__1861179588220582646.zip
2025-05-31 05:59:47,335 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:48,478 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:50,024 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2003/pyspark.zip
2025-05-31 05:59:50,043 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:50,091 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2003/py4j-0.10.9.7-src.zip
2025-05-31 05:59:50,100 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:50,324 INFO yarn.Client: Uploading resource file:/tmp/spark-5b331578-8b57-4254-9e1b-09bfccbcb821/__spark_conf__324454327558406001.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2003/__spark_conf__.zip
2025-05-31 05:59:50,340 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:50,410 INFO spark.SecurityManager: Changing view acls to: hadoop
2025-05-31 05:59:50,413 INFO spark.SecurityManager: Changing modify acls to: hadoop
2025-05-31 05:59:50,413 INFO spark.SecurityManager: Changing view acls groups to: 
2025-05-31 05:59:50,413 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-05-31 05:59:50,413 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
2025-05-31 05:59:50,449 INFO yarn.Client: Submitting application application_1747652871095_2003 to ResourceManager
2025-05-31 05:59:50,697 INFO impl.YarnClientImpl: Submitted application application_1747652871095_2003
2025-05-31 05:59:51,714 INFO yarn.Client: Application report for application_1747652871095_2003 (state: ACCEPTED)
2025-05-31 05:59:51,723 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1748671190482
	 final status: UNDEFINED
	 tracking URL: http://hadoop-namenode:8088/proxy/application_1747652871095_2003/
	 user: hadoop
2025-05-31 05:59:52,737 INFO yarn.Client: Application report for application_1747652871095_2003 (state: ACCEPTED)
2025-05-31 05:59:53,745 INFO yarn.Client: Application report for application_1747652871095_2003 (state: ACCEPTED)
2025-05-31 05:59:54,758 INFO yarn.Client: Application report for application_1747652871095_2003 (state: ACCEPTED)
2025-05-31 05:59:55,770 INFO yarn.Client: Application report for application_1747652871095_2003 (state: ACCEPTED)
2025-05-31 05:59:56,522 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop-namenode, PROXY_URI_BASES -> http://hadoop-namenode:8088/proxy/application_1747652871095_2003), /proxy/application_1747652871095_2003
2025-05-31 05:59:56,776 INFO yarn.Client: Application report for application_1747652871095_2003 (state: RUNNING)
2025-05-31 05:59:56,777 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.1.1.192
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1748671190482
	 final status: UNDEFINED
	 tracking URL: http://hadoop-namenode:8088/proxy/application_1747652871095_2003/
	 user: hadoop
2025-05-31 05:59:56,782 INFO cluster.YarnClientSchedulerBackend: Application application_1747652871095_2003 has started running.
2025-05-31 05:59:56,801 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38365.
2025-05-31 05:59:56,802 INFO netty.NettyBlockTransferService: Server created on hadoop-namenode:38365
2025-05-31 05:59:56,805 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-31 05:59:56,822 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop-namenode, 38365, None)
2025-05-31 05:59:56,829 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-namenode:38365 with 93.3 MiB RAM, BlockManagerId(driver, hadoop-namenode, 38365, None)
2025-05-31 05:59:56,842 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop-namenode, 38365, None)
2025-05-31 05:59:56,844 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop-namenode, 38365, None)
2025-05-31 05:59:57,565 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2025-05-31 05:59:58,243 INFO history.SingleEventLogFileWriter: Logging events to hdfs://hadoop-namenode:9820/spark-logs/application_1747652871095_2003.inprogress
2025-05-31 05:59:58,529 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 05:59:58,632 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@272805eb{/,null,STOPPED,@Spark}
2025-05-31 05:59:58,633 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@158e56e4{/jobs,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,654 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,656 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@550450b3{/jobs/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,656 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,658 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65d9dd72{/jobs/job,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,658 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,659 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a8b33ca{/jobs/job/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,659 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,660 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5033df32{/stages,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,660 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,670 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36600e73{/stages/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,670 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,679 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64385b59{/stages/stage,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,679 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,681 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14efced2{/stages/stage/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,681 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,682 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3efc0648{/stages/pool,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,682 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,683 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@215ebb57{/stages/pool/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,684 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,694 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fb7ffd8{/storage,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,694 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,696 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71922324{/storage/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,696 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,697 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15097040{/storage/rdd,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,698 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,699 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7541a7a0{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,699 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,700 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f133475{/environment,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,700 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,713 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@624f71d4{/environment/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,713 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,715 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59c287f8{/executors,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,715 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,716 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12f8e8a6{/executors/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,716 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,722 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4750a9b6{/executors/threadDump,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,722 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,724 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@449ab093{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,725 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,774 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26f182e7{/static,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,775 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,777 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@546576fb{/,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,777 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,782 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2706c9f6{/api,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,782 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,793 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b038fc7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,794 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,797 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10cf066d{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-05-31 05:59:58,814 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 05:59:58,815 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1483ef72{/metrics/json,null,AVAILABLE,@Spark}
2025-05-31 06:00:06,514 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.1.189:58616) with ID 1,  ResourceProfileId 0
2025-05-31 06:00:06,729 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-2:37519 with 93.3 MiB RAM, BlockManagerId(1, hadoop-datanode-2, 37519, None)
2025-05-31 06:00:12,772 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
2025-05-31 06:00:13,254 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-31 06:00:13,258 INFO internal.SharedState: Warehouse path is 'file:/home/hadoop/project/test100doc/spark-warehouse'.
2025-05-31 06:00:13,281 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 06:00:13,291 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44b39cce{/SQL,null,AVAILABLE,@Spark}
2025-05-31 06:00:13,292 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 06:00:13,300 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7494d808{/SQL/json,null,AVAILABLE,@Spark}
2025-05-31 06:00:13,301 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 06:00:13,303 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@166a8ae9{/SQL/execution,null,AVAILABLE,@Spark}
2025-05-31 06:00:13,303 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 06:00:13,305 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b0a3787{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-05-31 06:00:13,306 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 06:00:13,311 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55a031e4{/static/sql,null,AVAILABLE,@Spark}
2025-05-31 06:00:15,144 INFO datasources.InMemoryFileIndex: It took 178 ms to list leaf files for 1 paths.
2025-05-31 06:00:19,074 INFO datasources.FileSourceStrategy: Pushed Filters: 
2025-05-31 06:00:19,077 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
2025-05-31 06:00:19,340 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 385.0 KiB, free 92.9 MiB)
2025-05-31 06:00:19,445 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.6 KiB, free 92.9 MiB)
2025-05-31 06:00:19,453 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-namenode:38365 (size: 42.6 KiB, free: 93.3 MiB)
2025-05-31 06:00:19,473 INFO spark.SparkContext: Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0
2025-05-31 06:00:19,498 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
Number of partitions: 22
2025-05-31 06:00:19,986 INFO datasources.FileSourceStrategy: Pushed Filters: 
2025-05-31 06:00:19,986 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
2025-05-31 06:00:20,519 INFO codegen.CodeGenerator: Code generated in 366.426512 ms
2025-05-31 06:00:20,530 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 385.0 KiB, free 92.5 MiB)
2025-05-31 06:00:20,595 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 42.6 KiB, free 92.5 MiB)
2025-05-31 06:00:20,596 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-namenode:38365 (size: 42.6 KiB, free: 93.2 MiB)
2025-05-31 06:00:20,598 INFO spark.SparkContext: Created broadcast 1 from javaToPython at NativeMethodAccessorImpl.java:0
2025-05-31 06:00:20,601 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2025-05-31 06:00:20,967 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2025-05-31 06:00:20,977 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2025-05-31 06:00:20,980 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2025-05-31 06:00:20,980 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-05-31 06:00:21,018 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
2025-05-31 06:00:21,070 INFO scheduler.DAGScheduler: Registering RDD 12 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) as input to shuffle 1
2025-05-31 06:00:21,079 INFO scheduler.DAGScheduler: Registering RDD 16 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:58) as input to shuffle 0
2025-05-31 06:00:21,082 INFO scheduler.DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 22 output partitions
2025-05-31 06:00:21,082 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)
2025-05-31 06:00:21,083 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-31 06:00:21,087 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
2025-05-31 06:00:21,123 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[12] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53), which has no missing parents
2025-05-31 06:00:21,272 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.0 KiB, free 92.4 MiB)
2025-05-31 06:00:21,277 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 92.4 MiB)
2025-05-31 06:00:21,279 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-namenode:38365 (size: 12.3 KiB, free: 93.2 MiB)
2025-05-31 06:00:21,280 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1540
2025-05-31 06:00:21,307 INFO scheduler.DAGScheduler: Submitting 22 missing tasks from ShuffleMapStage 0 (PairwiseRDD[12] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2025-05-31 06:00:21,309 INFO cluster.YarnScheduler: Adding task set 0.0 with 22 tasks resource profile 0
2025-05-31 06:00:21,373 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop-datanode-2, executor 1, partition 0, NODE_LOCAL, 9586 bytes) 
2025-05-31 06:00:21,878 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-datanode-2:37519 (size: 12.3 KiB, free: 93.3 MiB)
2025-05-31 06:00:28,229 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-datanode-2:37519 (size: 42.6 KiB, free: 93.2 MiB)
2025-05-31 06:01:15,557 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hadoop-datanode-2, executor 1, partition 1, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:01:15,585 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 54225 ms on hadoop-datanode-2 (executor 1) (1/22)
2025-05-31 06:01:15,606 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44029
2025-05-31 06:01:57,509 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (hadoop-datanode-2, executor 1, partition 2, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:01:57,519 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 41966 ms on hadoop-datanode-2 (executor 1) (2/22)
2025-05-31 06:02:38,767 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (hadoop-datanode-2, executor 1, partition 3, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:02:38,777 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 41269 ms on hadoop-datanode-2 (executor 1) (3/22)
2025-05-31 06:03:22,699 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (hadoop-datanode-2, executor 1, partition 4, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:03:22,706 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 43941 ms on hadoop-datanode-2 (executor 1) (4/22)
2025-05-31 06:04:01,071 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (hadoop-datanode-2, executor 1, partition 5, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:04:01,075 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 38377 ms on hadoop-datanode-2 (executor 1) (5/22)
2025-05-31 06:04:43,528 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (hadoop-datanode-2, executor 1, partition 6, NODE_LOCAL, 9587 bytes) 
2025-05-31 06:04:43,531 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 42462 ms on hadoop-datanode-2 (executor 1) (6/22)
2025-05-31 06:05:19,361 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (hadoop-datanode-2, executor 1, partition 7, NODE_LOCAL, 9586 bytes) 
2025-05-31 06:05:19,365 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 35838 ms on hadoop-datanode-2 (executor 1) (7/22)
2025-05-31 06:05:55,189 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (hadoop-datanode-2, executor 1, partition 8, NODE_LOCAL, 9586 bytes) 
2025-05-31 06:05:55,194 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 35834 ms on hadoop-datanode-2 (executor 1) (8/22)
2025-05-31 06:06:30,495 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 35307 ms on hadoop-datanode-2 (executor 1) (9/22)
2025-05-31 06:06:30,501 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 9) (hadoop-datanode-2, executor 1, partition 10, NODE_LOCAL, 9712 bytes) 
2025-05-31 06:07:15,413 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 0.0 (TID 10) (hadoop-datanode-2, executor 1, partition 11, NODE_LOCAL, 9712 bytes) 
2025-05-31 06:07:15,415 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 9) in 44917 ms on hadoop-datanode-2 (executor 1) (10/22)
2025-05-31 06:08:01,628 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 0.0 (TID 11) (hadoop-datanode-2, executor 1, partition 12, NODE_LOCAL, 9712 bytes) 
2025-05-31 06:08:01,632 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 0.0 (TID 10) in 46221 ms on hadoop-datanode-2 (executor 1) (11/22)
2025-05-31 06:08:45,949 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 0.0 (TID 12) (hadoop-datanode-2, executor 1, partition 13, NODE_LOCAL, 9710 bytes) 
2025-05-31 06:08:45,951 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 0.0 (TID 11) in 44325 ms on hadoop-datanode-2 (executor 1) (12/22)
2025-05-31 06:09:28,999 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 0.0 (TID 13) (hadoop-datanode-2, executor 1, partition 14, NODE_LOCAL, 9712 bytes) 
2025-05-31 06:09:29,002 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 0.0 (TID 12) in 43057 ms on hadoop-datanode-2 (executor 1) (13/22)
2025-05-31 06:10:09,539 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 14) (hadoop-datanode-2, executor 1, partition 15, NODE_LOCAL, 9838 bytes) 
2025-05-31 06:10:09,542 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 0.0 (TID 13) in 40545 ms on hadoop-datanode-2 (executor 1) (14/22)
2025-05-31 06:10:56,111 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 0.0 (TID 15) (hadoop-datanode-2, executor 1, partition 16, NODE_LOCAL, 9962 bytes) 
2025-05-31 06:10:56,116 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 14) in 46586 ms on hadoop-datanode-2 (executor 1) (15/22)
2025-05-31 06:11:39,734 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 0.0 (TID 16) (hadoop-datanode-2, executor 1, partition 17, NODE_LOCAL, 10087 bytes) 
2025-05-31 06:11:39,738 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 0.0 (TID 15) in 43629 ms on hadoop-datanode-2 (executor 1) (16/22)
2025-05-31 06:12:26,161 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 0.0 (TID 17) (hadoop-datanode-2, executor 1, partition 18, NODE_LOCAL, 10212 bytes) 
2025-05-31 06:12:26,167 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 0.0 (TID 16) in 46439 ms on hadoop-datanode-2 (executor 1) (17/22)
2025-05-31 06:13:10,736 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 0.0 (TID 18) (hadoop-datanode-2, executor 1, partition 19, NODE_LOCAL, 10336 bytes) 
2025-05-31 06:13:10,737 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 0.0 (TID 17) in 44577 ms on hadoop-datanode-2 (executor 1) (18/22)
2025-05-31 06:13:45,825 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 0.0 (TID 19) (hadoop-datanode-2, executor 1, partition 20, NODE_LOCAL, 10959 bytes) 
2025-05-31 06:13:45,831 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 0.0 (TID 18) in 35097 ms on hadoop-datanode-2 (executor 1) (19/22)
2025-05-31 06:14:23,542 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 0.0 (TID 20) (hadoop-datanode-2, executor 1, partition 21, NODE_LOCAL, 9337 bytes) 
2025-05-31 06:14:23,546 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 0.0 (TID 19) in 37727 ms on hadoop-datanode-2 (executor 1) (20/22)
2025-05-31 06:14:24,152 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 0.0 (TID 20) in 612 ms on hadoop-datanode-2 (executor 1) (21/22)
2025-05-31 06:14:26,778 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 21) (hadoop-datanode-2, executor 1, partition 9, RACK_LOCAL, 9587 bytes) 
2025-05-31 06:14:59,816 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 21) in 33056 ms on hadoop-datanode-2 (executor 1) (22/22)
2025-05-31 06:14:59,828 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-05-31 06:14:59,834 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) finished in 878.629 s
2025-05-31 06:14:59,837 INFO scheduler.DAGScheduler: looking for newly runnable stages
2025-05-31 06:14:59,838 INFO scheduler.DAGScheduler: running: Set()
2025-05-31 06:14:59,848 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
2025-05-31 06:14:59,848 INFO scheduler.DAGScheduler: failed: Set()
2025-05-31 06:14:59,859 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[16] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:58), which has no missing parents
2025-05-31 06:14:59,889 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.3 KiB, free 92.4 MiB)
2025-05-31 06:14:59,896 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 92.4 MiB)
2025-05-31 06:14:59,898 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-namenode:38365 (size: 7.2 KiB, free: 93.2 MiB)
2025-05-31 06:14:59,899 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1540
2025-05-31 06:14:59,900 INFO scheduler.DAGScheduler: Submitting 22 missing tasks from ShuffleMapStage 1 (PairwiseRDD[16] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:58) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2025-05-31 06:14:59,900 INFO cluster.YarnScheduler: Adding task set 1.0 with 22 tasks resource profile 0
2025-05-31 06:14:59,904 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 22) (hadoop-datanode-2, executor 1, partition 0, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:14:59,967 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop-datanode-2:37519 (size: 7.2 KiB, free: 93.2 MiB)
2025-05-31 06:15:00,023 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.1.1.189:58616
2025-05-31 06:15:07,293 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 23) (hadoop-datanode-2, executor 1, partition 1, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:07,298 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 22) in 7396 ms on hadoop-datanode-2 (executor 1) (1/22)
2025-05-31 06:15:14,265 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 24) (hadoop-datanode-2, executor 1, partition 2, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:14,270 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 23) in 6978 ms on hadoop-datanode-2 (executor 1) (2/22)
2025-05-31 06:15:21,478 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 25) (hadoop-datanode-2, executor 1, partition 3, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:21,482 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 24) in 7218 ms on hadoop-datanode-2 (executor 1) (3/22)
2025-05-31 06:15:28,900 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 26) (hadoop-datanode-2, executor 1, partition 4, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:28,906 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 25) in 7429 ms on hadoop-datanode-2 (executor 1) (4/22)
2025-05-31 06:15:35,680 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 27) (hadoop-datanode-2, executor 1, partition 5, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:35,685 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 26) in 6786 ms on hadoop-datanode-2 (executor 1) (5/22)
2025-05-31 06:15:42,464 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 28) (hadoop-datanode-2, executor 1, partition 6, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:42,473 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 27) in 6794 ms on hadoop-datanode-2 (executor 1) (6/22)
2025-05-31 06:15:49,503 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 29) (hadoop-datanode-2, executor 1, partition 7, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:49,507 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 28) in 7044 ms on hadoop-datanode-2 (executor 1) (7/22)
2025-05-31 06:15:56,536 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 30) (hadoop-datanode-2, executor 1, partition 8, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:15:56,540 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 29) in 7039 ms on hadoop-datanode-2 (executor 1) (8/22)
2025-05-31 06:16:03,172 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 30) in 6636 ms on hadoop-datanode-2 (executor 1) (9/22)
2025-05-31 06:16:03,175 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 31) (hadoop-datanode-2, executor 1, partition 9, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:10,224 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 1.0 (TID 32) (hadoop-datanode-2, executor 1, partition 10, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:10,229 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 31) in 7054 ms on hadoop-datanode-2 (executor 1) (10/22)
2025-05-31 06:16:17,190 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 1.0 (TID 33) (hadoop-datanode-2, executor 1, partition 11, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:17,194 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 1.0 (TID 32) in 6976 ms on hadoop-datanode-2 (executor 1) (11/22)
2025-05-31 06:16:24,416 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 1.0 (TID 33) in 7226 ms on hadoop-datanode-2 (executor 1) (12/22)
2025-05-31 06:16:24,424 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 1.0 (TID 34) (hadoop-datanode-2, executor 1, partition 12, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:31,436 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 35) (hadoop-datanode-2, executor 1, partition 13, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:31,441 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 1.0 (TID 34) in 7018 ms on hadoop-datanode-2 (executor 1) (13/22)
2025-05-31 06:16:38,472 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 1.0 (TID 36) (hadoop-datanode-2, executor 1, partition 14, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:38,475 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 1.0 (TID 35) in 7041 ms on hadoop-datanode-2 (executor 1) (14/22)
2025-05-31 06:16:45,187 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 1.0 (TID 37) (hadoop-datanode-2, executor 1, partition 15, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:45,192 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 1.0 (TID 36) in 6721 ms on hadoop-datanode-2 (executor 1) (15/22)
2025-05-31 06:16:52,080 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 1.0 (TID 38) (hadoop-datanode-2, executor 1, partition 16, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:52,081 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 1.0 (TID 37) in 6896 ms on hadoop-datanode-2 (executor 1) (16/22)
2025-05-31 06:16:58,965 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 1.0 (TID 39) (hadoop-datanode-2, executor 1, partition 17, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:16:58,969 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 1.0 (TID 38) in 6891 ms on hadoop-datanode-2 (executor 1) (17/22)
2025-05-31 06:17:05,807 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 1.0 (TID 40) (hadoop-datanode-2, executor 1, partition 18, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:17:05,817 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 1.0 (TID 39) in 6856 ms on hadoop-datanode-2 (executor 1) (18/22)
2025-05-31 06:17:18,400 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 41) (hadoop-datanode-2, executor 1, partition 19, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:17:18,405 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 1.0 (TID 40) in 12600 ms on hadoop-datanode-2 (executor 1) (19/22)
2025-05-31 06:17:25,831 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 1.0 (TID 42) (hadoop-datanode-2, executor 1, partition 20, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:17:25,843 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 1.0 (TID 41) in 7445 ms on hadoop-datanode-2 (executor 1) (20/22)
2025-05-31 06:17:34,035 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 1.0 (TID 43) (hadoop-datanode-2, executor 1, partition 21, NODE_LOCAL, 8565 bytes) 
2025-05-31 06:17:34,040 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 1.0 (TID 42) in 8214 ms on hadoop-datanode-2 (executor 1) (21/22)
2025-05-31 06:17:41,113 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 1.0 (TID 43) in 7080 ms on hadoop-datanode-2 (executor 1) (22/22)
2025-05-31 06:17:41,114 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-05-31 06:17:41,116 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:58) finished in 161.238 s
2025-05-31 06:17:41,117 INFO scheduler.DAGScheduler: looking for newly runnable stages
2025-05-31 06:17:41,117 INFO scheduler.DAGScheduler: running: Set()
2025-05-31 06:17:41,117 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
2025-05-31 06:17:41,125 INFO scheduler.DAGScheduler: failed: Set()
2025-05-31 06:17:41,127 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents
2025-05-31 06:17:41,173 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 126.2 KiB, free 92.3 MiB)
2025-05-31 06:17:41,180 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 48.6 KiB, free 92.2 MiB)
2025-05-31 06:17:41,182 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-namenode:38365 (size: 48.6 KiB, free: 93.2 MiB)
2025-05-31 06:17:41,183 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1540
2025-05-31 06:17:41,187 INFO scheduler.DAGScheduler: Submitting 22 missing tasks from ResultStage 2 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2025-05-31 06:17:41,187 INFO cluster.YarnScheduler: Adding task set 2.0 with 22 tasks resource profile 0
2025-05-31 06:17:41,193 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 44) (hadoop-datanode-2, executor 1, partition 0, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:17:41,238 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop-datanode-2:37519 (size: 48.6 KiB, free: 93.2 MiB)
2025-05-31 06:17:41,328 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.1.1.189:58616
2025-05-31 06:17:47,182 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 45) (hadoop-datanode-2, executor 1, partition 1, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:17:47,188 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 44) in 5997 ms on hadoop-datanode-2 (executor 1) (1/22)
2025-05-31 06:17:51,670 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 46) (hadoop-datanode-2, executor 1, partition 2, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:17:51,676 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 45) in 4495 ms on hadoop-datanode-2 (executor 1) (2/22)
2025-05-31 06:18:01,669 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 47) (hadoop-datanode-2, executor 1, partition 3, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:01,672 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 46) in 10005 ms on hadoop-datanode-2 (executor 1) (3/22)
2025-05-31 06:18:05,778 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 48) (hadoop-datanode-2, executor 1, partition 4, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:05,782 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 47) in 4115 ms on hadoop-datanode-2 (executor 1) (4/22)
2025-05-31 06:18:09,823 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 49) (hadoop-datanode-2, executor 1, partition 5, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:09,827 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 48) in 4050 ms on hadoop-datanode-2 (executor 1) (5/22)
2025-05-31 06:18:14,858 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 2.0 (TID 50) (hadoop-datanode-2, executor 1, partition 6, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:14,861 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 49) in 5039 ms on hadoop-datanode-2 (executor 1) (6/22)
2025-05-31 06:18:20,512 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 2.0 (TID 51) (hadoop-datanode-2, executor 1, partition 7, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:20,516 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 2.0 (TID 50) in 5659 ms on hadoop-datanode-2 (executor 1) (7/22)
2025-05-31 06:18:24,811 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 2.0 (TID 52) (hadoop-datanode-2, executor 1, partition 8, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:24,817 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 2.0 (TID 51) in 4306 ms on hadoop-datanode-2 (executor 1) (8/22)
2025-05-31 06:18:28,859 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 2.0 (TID 52) in 4050 ms on hadoop-datanode-2 (executor 1) (9/22)
2025-05-31 06:18:28,861 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 2.0 (TID 53) (hadoop-datanode-2, executor 1, partition 9, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:32,958 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 2.0 (TID 54) (hadoop-datanode-2, executor 1, partition 10, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:32,961 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 2.0 (TID 53) in 4101 ms on hadoop-datanode-2 (executor 1) (10/22)
2025-05-31 06:18:39,143 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 2.0 (TID 55) (hadoop-datanode-2, executor 1, partition 11, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:39,150 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 2.0 (TID 54) in 6193 ms on hadoop-datanode-2 (executor 1) (11/22)
2025-05-31 06:18:43,589 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 2.0 (TID 56) (hadoop-datanode-2, executor 1, partition 12, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:43,592 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 2.0 (TID 55) in 4450 ms on hadoop-datanode-2 (executor 1) (12/22)
2025-05-31 06:18:50,991 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 2.0 (TID 57) (hadoop-datanode-2, executor 1, partition 13, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:50,993 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 2.0 (TID 56) in 7405 ms on hadoop-datanode-2 (executor 1) (13/22)
2025-05-31 06:18:55,320 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 2.0 (TID 57) in 4329 ms on hadoop-datanode-2 (executor 1) (14/22)
2025-05-31 06:18:55,322 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 2.0 (TID 58) (hadoop-datanode-2, executor 1, partition 14, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:59,336 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 2.0 (TID 59) (hadoop-datanode-2, executor 1, partition 15, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:18:59,340 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 2.0 (TID 58) in 4018 ms on hadoop-datanode-2 (executor 1) (15/22)
2025-05-31 06:19:03,688 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 2.0 (TID 60) (hadoop-datanode-2, executor 1, partition 16, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:03,698 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 2.0 (TID 59) in 4364 ms on hadoop-datanode-2 (executor 1) (16/22)
2025-05-31 06:19:11,270 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 2.0 (TID 61) (hadoop-datanode-2, executor 1, partition 17, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:11,274 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 2.0 (TID 60) in 7588 ms on hadoop-datanode-2 (executor 1) (17/22)
2025-05-31 06:19:15,670 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 2.0 (TID 62) (hadoop-datanode-2, executor 1, partition 18, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:15,675 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 2.0 (TID 61) in 4405 ms on hadoop-datanode-2 (executor 1) (18/22)
2025-05-31 06:19:19,866 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 2.0 (TID 63) (hadoop-datanode-2, executor 1, partition 19, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:19,869 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 2.0 (TID 62) in 4201 ms on hadoop-datanode-2 (executor 1) (19/22)
2025-05-31 06:19:26,711 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 2.0 (TID 64) (hadoop-datanode-2, executor 1, partition 20, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:26,725 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 2.0 (TID 63) in 6861 ms on hadoop-datanode-2 (executor 1) (20/22)
2025-05-31 06:19:30,857 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 2.0 (TID 64) in 4146 ms on hadoop-datanode-2 (executor 1) (21/22)
2025-05-31 06:19:30,860 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 2.0 (TID 65) (hadoop-datanode-2, executor 1, partition 21, NODE_LOCAL, 8576 bytes) 
2025-05-31 06:19:36,822 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 2.0 (TID 65) in 5963 ms on hadoop-datanode-2 (executor 1) (22/22)
2025-05-31 06:19:36,823 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
2025-05-31 06:19:36,828 INFO scheduler.DAGScheduler: ResultStage 2 (runJob at SparkHadoopWriter.scala:83) finished in 115.693 s
2025-05-31 06:19:36,846 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-05-31 06:19:36,846 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished
2025-05-31 06:19:36,854 INFO scheduler.DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 1155.835837 s
2025-05-31 06:19:36,861 INFO io.SparkHadoopWriter: Start to commit write Job job_202505310600202202577455970555415_0021.
2025-05-31 06:19:37,053 INFO io.SparkHadoopWriter: Write Job job_202505310600202202577455970555415_0021 committed. Elapsed time: 190 ms.
2025-05-31 06:19:37,056 INFO spark.SparkContext: SparkContext is stopping with exitCode 0.
2025-05-31 06:19:37,074 INFO server.AbstractConnector: Stopped Spark@690170d4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-05-31 06:19:37,080 INFO ui.SparkUI: Stopped Spark web UI at http://hadoop-namenode:4040
2025-05-31 06:19:37,092 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
2025-05-31 06:19:37,174 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
2025-05-31 06:19:37,175 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
2025-05-31 06:19:37,188 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped
2025-05-31 06:19:37,281 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-31 06:19:37,312 INFO memory.MemoryStore: MemoryStore cleared
2025-05-31 06:19:37,313 INFO storage.BlockManager: BlockManager stopped
2025-05-31 06:19:37,323 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
2025-05-31 06:19:37,330 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-31 06:19:37,349 INFO spark.SparkContext: Successfully stopped SparkContext
2025-05-31 06:19:37,938 INFO util.ShutdownHookManager: Shutdown hook called
2025-05-31 06:19:37,940 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5b331578-8b57-4254-9e1b-09bfccbcb821/pyspark-bca7308d-7306-469c-8edd-8aed2bb45c95
2025-05-31 06:19:37,946 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-240ef386-3958-434c-a0f7-dea48324bf1d
2025-05-31 06:19:37,950 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5b331578-8b57-4254-9e1b-09bfccbcb821
