2025-05-31 10:46:06,131 INFO spark.SparkContext: Running Spark version 3.4.4
2025-05-31 10:46:06,498 INFO resource.ResourceUtils: ==============================================================
2025-05-31 10:46:06,499 INFO resource.ResourceUtils: No custom resources configured for spark.driver.
2025-05-31 10:46:06,499 INFO resource.ResourceUtils: ==============================================================
2025-05-31 10:46:06,499 INFO spark.SparkContext: Submitted application: invertedIndex
2025-05-31 10:46:06,539 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-05-31 10:46:06,566 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor
2025-05-31 10:46:06,574 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0
2025-05-31 10:46:06,692 INFO spark.SecurityManager: Changing view acls to: hadoop
2025-05-31 10:46:06,697 INFO spark.SecurityManager: Changing modify acls to: hadoop
2025-05-31 10:46:06,698 INFO spark.SecurityManager: Changing view acls groups to: 
2025-05-31 10:46:06,699 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-05-31 10:46:06,699 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
2025-05-31 10:46:07,207 INFO util.Utils: Successfully started service 'sparkDriver' on port 43435.
2025-05-31 10:46:07,291 INFO spark.SparkEnv: Registering MapOutputTracker
2025-05-31 10:46:07,353 INFO spark.SparkEnv: Registering BlockManagerMaster
2025-05-31 10:46:07,396 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-05-31 10:46:07,398 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
2025-05-31 10:46:07,473 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
2025-05-31 10:46:07,537 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f42a8834-c3bc-4d61-a333-134bddc60f86
2025-05-31 10:46:07,576 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MiB
2025-05-31 10:46:07,653 INFO spark.SparkEnv: Registering OutputCommitCoordinator
2025-05-31 10:46:07,763 INFO util.log: Logging initialized @4907ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-05-31 10:46:07,961 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
2025-05-31 10:46:07,988 INFO server.Server: jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 1.8.0_452-8u452-ga~us1-0ubuntu1~22.04-b09
2025-05-31 10:46:08,031 INFO server.Server: Started @5177ms
2025-05-31 10:46:08,100 INFO server.AbstractConnector: Started ServerConnector@6a4cabd4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-05-31 10:46:08,101 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
2025-05-31 10:46:08,236 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@462f8e1d{/,null,AVAILABLE,@Spark}
2025-05-31 10:46:09,057 INFO util.Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
2025-05-31 10:46:09,224 INFO client.RMProxy: Connecting to ResourceManager at hadoop-namenode/10.1.1.183:8032
2025-05-31 10:46:10,671 INFO conf.Configuration: resource-types.xml not found
2025-05-31 10:46:10,674 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-05-31 10:46:10,699 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container)
2025-05-31 10:46:10,700 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
2025-05-31 10:46:10,700 INFO yarn.Client: Setting up container launch context for our AM
2025-05-31 10:46:10,703 INFO yarn.Client: Setting up the launch environment for our AM container
2025-05-31 10:46:10,719 INFO yarn.Client: Preparing resources for our AM container
2025-05-31 10:46:10,785 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2025-05-31 10:46:27,653 INFO yarn.Client: Uploading resource file:/tmp/spark-ca937e8c-3bfa-4541-897c-a2503f10f4e4/__spark_libs__1115591323279788141.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2014/__spark_libs__1115591323279788141.zip
2025-05-31 10:46:27,895 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:31,085 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:31,894 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2014/pyspark.zip
2025-05-31 10:46:31,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:31,987 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2014/py4j-0.10.9.7-src.zip
2025-05-31 10:46:31,996 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:32,237 INFO yarn.Client: Uploading resource file:/tmp/spark-ca937e8c-3bfa-4541-897c-a2503f10f4e4/__spark_conf__7042692912299772789.zip -> hdfs://hadoop-namenode:9820/user/hadoop/.sparkStaging/application_1747652871095_2014/__spark_conf__.zip
2025-05-31 10:46:32,248 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:32,435 INFO spark.SecurityManager: Changing view acls to: hadoop
2025-05-31 10:46:32,436 INFO spark.SecurityManager: Changing modify acls to: hadoop
2025-05-31 10:46:32,436 INFO spark.SecurityManager: Changing view acls groups to: 
2025-05-31 10:46:32,436 INFO spark.SecurityManager: Changing modify acls groups to: 
2025-05-31 10:46:32,436 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
2025-05-31 10:46:32,475 INFO yarn.Client: Submitting application application_1747652871095_2014 to ResourceManager
2025-05-31 10:46:32,573 INFO impl.YarnClientImpl: Submitted application application_1747652871095_2014
2025-05-31 10:46:33,589 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:33,605 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1748688392517
	 final status: UNDEFINED
	 tracking URL: http://hadoop-namenode:8088/proxy/application_1747652871095_2014/
	 user: hadoop
2025-05-31 10:46:34,612 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:35,629 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:36,636 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:37,649 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:38,660 INFO yarn.Client: Application report for application_1747652871095_2014 (state: ACCEPTED)
2025-05-31 10:46:39,591 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop-namenode, PROXY_URI_BASES -> http://hadoop-namenode:8088/proxy/application_1747652871095_2014), /proxy/application_1747652871095_2014
2025-05-31 10:46:39,669 INFO yarn.Client: Application report for application_1747652871095_2014 (state: RUNNING)
2025-05-31 10:46:39,672 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.1.1.192
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1748688392517
	 final status: UNDEFINED
	 tracking URL: http://hadoop-namenode:8088/proxy/application_1747652871095_2014/
	 user: hadoop
2025-05-31 10:46:39,681 INFO cluster.YarnClientSchedulerBackend: Application application_1747652871095_2014 has started running.
2025-05-31 10:46:39,721 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46369.
2025-05-31 10:46:39,721 INFO netty.NettyBlockTransferService: Server created on hadoop-namenode:46369
2025-05-31 10:46:39,726 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-05-31 10:46:39,742 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop-namenode, 46369, None)
2025-05-31 10:46:39,750 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-namenode:46369 with 93.3 MiB RAM, BlockManagerId(driver, hadoop-namenode, 46369, None)
2025-05-31 10:46:39,754 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop-namenode, 46369, None)
2025-05-31 10:46:39,756 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop-namenode, 46369, None)
2025-05-31 10:46:40,118 INFO history.SingleEventLogFileWriter: Logging events to hdfs://hadoop-namenode:9820/spark-logs/application_1747652871095_2014.inprogress
2025-05-31 10:46:40,361 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2025-05-31 10:46:40,406 INFO util.Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
2025-05-31 10:46:40,407 INFO spark.ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.
2025-05-31 10:46:40,432 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
2025-05-31 10:46:40,504 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@462f8e1d{/,null,STOPPED,@Spark}
2025-05-31 10:46:40,514 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,549 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3af9ff80{/jobs,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,550 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,555 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43f6cee9{/jobs/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,555 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,559 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c1dfb9c{/jobs/job,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,562 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,565 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d3b2cf3{/jobs/job/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,566 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,569 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d0c6d73{/stages,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,573 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,576 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@644c105e{/stages/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,580 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,590 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e328b99{/stages/stage,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,591 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,592 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18322209{/stages/stage/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,605 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,606 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43a71102{/stages/pool,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,607 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,608 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e63ab7b{/stages/pool/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,609 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,610 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2de29f95{/storage,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,612 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,613 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30a7fed0{/storage/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,614 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,615 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b83bf10{/storage/rdd,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,620 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,624 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6619e4ee{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,625 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,626 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c8caf99{/environment,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,629 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,630 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42a62a1f{/environment/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,631 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,632 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25900f84{/executors,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,636 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,638 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64c1d03{/executors/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,638 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@551cfbac{/executors/threadDump,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,641 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,643 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64d776b3{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,644 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,659 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@639a327{/static,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,660 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,662 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fe54cc3{/,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,662 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,667 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@171d3951{/api,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,669 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,672 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@631e204{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,672 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,677 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b1e66f6{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,684 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:40,685 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b4f2be9{/metrics/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:40,686 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
2025-05-31 10:46:40,745 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
2025-05-31 10:46:41,273 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-05-31 10:46:41,277 INFO internal.SharedState: Warehouse path is 'file:/home/hadoop/project/spark/spark-warehouse'.
2025-05-31 10:46:41,314 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:41,317 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e7a779e{/SQL,null,AVAILABLE,@Spark}
2025-05-31 10:46:41,318 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:41,319 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f42b7a2{/SQL/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:41,320 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:41,322 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76a1a532{/SQL/execution,null,AVAILABLE,@Spark}
2025-05-31 10:46:41,322 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:41,323 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@384d68c2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-05-31 10:46:41,325 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
2025-05-31 10:46:41,327 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ae726cf{/static/sql,null,AVAILABLE,@Spark}
2025-05-31 10:46:42,938 INFO datasources.InMemoryFileIndex: It took 146 ms to list leaf files for 1 paths.
2025-05-31 10:46:46,911 INFO datasources.FileSourceStrategy: Pushed Filters: 
2025-05-31 10:46:46,920 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
2025-05-31 10:46:47,152 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 385.4 KiB, free 92.9 MiB)
2025-05-31 10:46:47,248 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.6 KiB, free 92.9 MiB)
2025-05-31 10:46:47,252 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-namenode:46369 (size: 42.6 KiB, free: 93.3 MiB)
2025-05-31 10:46:47,261 INFO spark.SparkContext: Created broadcast 0 from javaToPython at NativeMethodAccessorImpl.java:0
2025-05-31 10:46:47,285 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
Number of partitions: 17
2025-05-31 10:46:47,740 INFO datasources.FileSourceStrategy: Pushed Filters: 
2025-05-31 10:46:47,741 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
2025-05-31 10:46:48,431 INFO codegen.CodeGenerator: Code generated in 530.098762 ms
2025-05-31 10:46:48,452 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 385.4 KiB, free 92.5 MiB)
2025-05-31 10:46:48,475 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 42.6 KiB, free 92.5 MiB)
2025-05-31 10:46:48,476 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-namenode:46369 (size: 42.6 KiB, free: 93.2 MiB)
2025-05-31 10:46:48,482 INFO spark.SparkContext: Created broadcast 1 from javaToPython at NativeMethodAccessorImpl.java:0
2025-05-31 10:46:48,484 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2025-05-31 10:46:48,854 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2025-05-31 10:46:48,864 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2025-05-31 10:46:48,867 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2025-05-31 10:46:48,867 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-05-31 10:46:48,923 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
2025-05-31 10:46:48,989 INFO scheduler.DAGScheduler: Registering RDD 12 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) as input to shuffle 1
2025-05-31 10:46:49,000 INFO scheduler.DAGScheduler: Registering RDD 16 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:58) as input to shuffle 0
2025-05-31 10:46:49,005 INFO scheduler.DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 17 output partitions
2025-05-31 10:46:49,007 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (runJob at SparkHadoopWriter.scala:83)
2025-05-31 10:46:49,007 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
2025-05-31 10:46:49,013 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
2025-05-31 10:46:49,054 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[12] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53), which has no missing parents
2025-05-31 10:46:49,342 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.0 KiB, free 92.4 MiB)
2025-05-31 10:46:49,348 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 92.4 MiB)
2025-05-31 10:46:49,350 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-namenode:46369 (size: 12.3 KiB, free: 93.2 MiB)
2025-05-31 10:46:49,352 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1540
2025-05-31 10:46:49,355 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)
2025-05-31 10:46:49,385 INFO scheduler.DAGScheduler: Submitting 17 missing tasks from ShuffleMapStage 0 (PairwiseRDD[12] at reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2025-05-31 10:46:49,387 INFO cluster.YarnScheduler: Adding task set 0.0 with 17 tasks resource profile 0
2025-05-31 10:46:50,291 INFO spark.ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 3 for resource profile id: 0)
2025-05-31 10:46:51,341 INFO spark.ExecutorAllocationManager: Requesting 4 new executors because tasks are backlogged (new desired total will be 7 for resource profile id: 0)
2025-05-31 10:46:52,373 INFO spark.ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 10 for resource profile id: 0)
2025-05-31 10:46:57,842 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.1.1.189:43356) with ID 1,  ResourceProfileId 0
2025-05-31 10:46:57,866 INFO dynalloc.ExecutorMonitor: New executor 1 has registered (new total is 1)
2025-05-31 10:46:58,040 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode-2:45231 with 93.3 MiB RAM, BlockManagerId(1, hadoop-datanode-2, 45231, None)
2025-05-31 10:46:58,182 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 0) (hadoop-datanode-2, executor 1, partition 5, NODE_LOCAL, 9335 bytes) 
2025-05-31 10:46:58,675 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop-datanode-2:45231 (size: 12.3 KiB, free: 93.3 MiB)
2025-05-31 10:47:04,172 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop-datanode-2:45231 (size: 42.6 KiB, free: 93.2 MiB)
2025-05-31 10:47:51,330 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 1) (hadoop-datanode-2, executor 1, partition 7, NODE_LOCAL, 9335 bytes) 
2025-05-31 10:47:51,362 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 0) in 53210 ms on hadoop-datanode-2 (executor 1) (1/17)
2025-05-31 10:47:51,380 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51637
2025-05-31 10:48:36,428 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 2) (hadoop-datanode-2, executor 1, partition 8, NODE_LOCAL, 9335 bytes) 
2025-05-31 10:48:36,441 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 1) in 45113 ms on hadoop-datanode-2 (executor 1) (2/17)
2025-05-31 10:49:22,691 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 3) (hadoop-datanode-2, executor 1, partition 9, NODE_LOCAL, 9335 bytes) 
2025-05-31 10:49:22,701 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 2) in 46274 ms on hadoop-datanode-2 (executor 1) (3/17)
2025-05-31 10:50:07,060 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 4) (hadoop-datanode-2, executor 1, partition 10, NODE_LOCAL, 9335 bytes) 
2025-05-31 10:50:07,070 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 3) in 44380 ms on hadoop-datanode-2 (executor 1) (4/17)
2025-05-31 10:50:52,020 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 5) (hadoop-datanode-2, executor 1, partition 15, NODE_LOCAL, 9458 bytes) 
2025-05-31 10:50:52,028 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 4) in 44970 ms on hadoop-datanode-2 (executor 1) (5/17)
2025-05-31 10:51:35,166 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 5) in 43151 ms on hadoop-datanode-2 (executor 1) (6/17)
2025-05-31 10:51:35,175 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 0.0 (TID 6) (hadoop-datanode-2, executor 1, partition 16, NODE_LOCAL, 9951 bytes) 
2025-05-31 10:52:14,685 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 7) (hadoop-datanode-2, executor 1, partition 0, RACK_LOCAL, 9335 bytes) 
2025-05-31 10:52:14,690 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 0.0 (TID 6) in 39519 ms on hadoop-datanode-2 (executor 1) (7/17)
2025-05-31 10:53:00,727 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 8) (hadoop-datanode-2, executor 1, partition 1, RACK_LOCAL, 9335 bytes) 
2025-05-31 10:53:00,739 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 7) in 46071 ms on hadoop-datanode-2 (executor 1) (8/17)
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o25.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-31 10:53:02,381 INFO spark.SparkContext: Invoking stop() from shutdown hook
2025-05-31 10:53:02,382 INFO spark.SparkContext: SparkContext is stopping with exitCode 0.
2025-05-31 10:53:02,417 INFO server.AbstractConnector: Stopped Spark@6a4cabd4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-05-31 10:53:02,438 INFO ui.SparkUI: Stopped Spark web UI at http://hadoop-namenode:4040
2025-05-31 10:53:02,452 INFO scheduler.DAGScheduler: Job 0 failed: runJob at SparkHadoopWriter.scala:83, took 373.528591 s
Traceback (most recent call last):
  File "/home/hadoop/project/spark/sparkInvertedIndex2.py", line 68, in <module>
    invertedIndexFormatted.saveAsTextFile(output_path)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py", line 3406, in saveAsTextFile
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o86.saveAsTextFile
2025-05-31 10:53:02,458 ERROR io.SparkHadoopWriter: Aborting job job_202505311046484145436205007853123_0021.
org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1217)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1215)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1215)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3016)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2907)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2907)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2123)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2123)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2076)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:659)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2311)
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)
	at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)
	at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1596)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1596)
	at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1582)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:408)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1582)
	at org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)
	at org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)
	at org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)
2025-05-31 10:53:02,465 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/hadoop/project/spark/sparkInvertedIndex2.py:53) failed in 373.318 s due to Stage cancelled because SparkContext was shut down
2025-05-31 10:53:02,508 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
2025-05-31 10:53:02,544 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
2025-05-31 10:53:02,547 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
2025-05-31 10:53:02,567 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped
2025-05-31 10:53:02,696 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
2025-05-31 10:53:02,742 INFO memory.MemoryStore: MemoryStore cleared
2025-05-31 10:53:02,747 INFO storage.BlockManager: BlockManager stopped
2025-05-31 10:53:02,761 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
2025-05-31 10:53:02,770 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
2025-05-31 10:53:02,791 INFO spark.SparkContext: Successfully stopped SparkContext
2025-05-31 10:53:02,791 INFO util.ShutdownHookManager: Shutdown hook called
2025-05-31 10:53:02,793 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ca937e8c-3bfa-4541-897c-a2503f10f4e4
2025-05-31 10:53:02,799 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-77955ff8-773c-4a72-a45c-e3e4334f5e6a
2025-05-31 10:53:02,804 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ca937e8c-3bfa-4541-897c-a2503f10f4e4/pyspark-0dee9f31-8546-44a1-aa7b-f4cb882b1723
